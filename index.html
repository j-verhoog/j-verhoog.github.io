<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- ADJUSTMENT: Added description meta for SEO and quick pitch -->
    <meta name="description" content="Jolle Verhoog – Robotics & Computer Vision Engineer. Explore projects in automation, intelligent systems, and computer vision.">
    <title>Jolle Verhoog – Robotics & Computer Vision Engineer</title>
    <link rel="icon" href="/images/favicon_robot.png" type="image/png">
    <link rel="stylesheet" href="/styles.css">
</head>
<body>

    <!-- Header Bar -->
    <header>
        <div class="social-links">
            <a href="https://github.com/j-verhoog" target="_blank">GitHub</a>
            <a href="https://www.linkedin.com/in/jolle-verhoog-77b9b21b6/" target="_blank">LinkedIn</a>
            <a href="mailto:permanent.beleefd6q@icloud.com">Contact</a>
        </div>
        <div class="project-dropdown">
                <!-- ADJUSTMENT: Simplified label from 'All Projects' to 'Projects' -->
                <button id="menu-toggle" class="menu-btn">Projects ▼</button>
                <ul id="project-menu" class="dropdown-menu">
                  <li class="dropdown-header">Academic Projects</li>
                  <li><a href="#master-thesis">MSc Thesis – Federated Learning</a></li>
                  <li><a href="#apple-robot">MSc Final Project – Autonomous Apple Picking Robot</a></li>
                  <li><a href="#internship-n-sea">MSc Internship – N-Sea</a></li>
                  <li><a href="#adv-machine-perception">Advanced Machine Perception</a></li>
                  <li><a href="#deep-learning">Deep Learning (for Computer Vision)</a></li>
                  <li><a href="#skeleton-annotation">BSc Thesis – Skeleton Annotation</a></li>

                  <li class="divider"></li>
                  <li class="dropdown-header">Personal Projects</li>
                  <li><a href="/projects/skeleton-tag/">Skeleton Tag</a></li>
                  <li><a href="/projects/snake-js/">Snake Game</a></li>
                  
                </ul>
            </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- Profile Section -->
        <section class="profile">
            <img src="/images/profile_pic.jpg" alt="Your Photo" class="profile-photo">
            <div class="profile-description">
                <h1>Jolle Verhoog</h1>
                <p>
                    Hi, I am Jolle Verhoog and welcome to my portfolio! 
                    I am a Robotics Engineer with a deep interest in automation, intelligent systems and computer vision.
                    This portfolio highlights my journey through innovative projects, demonstrating how I apply cutting-edge technologies to solve real-world challenges. 
                    <br>
                    Feel free to explore and to reach out!
                </p>
            </div>
        </section>

        <section class="projects">

            <!-- Master Thesis -->
            <div id="master-thesis" class="project">
                <h2>MSc Thesis – Federated Learning for Perception in Autonomous Driving</h2>
                <p><em>More details coming soon...</em></p>
            </div>




            <!-- Autonomous Apple Picking Robot -->
            <div id="apple-robot" class="project">
                <h2>MSc Final Project - Autonomous Apple Picking Robot</h2>
                <p class="impact"><strong><em>Helping farmers pick apples 24/7 without extra manual labor.</em></strong></p>
                <div class="project-media">
                    <img src="/images/JTW07211.JPG" alt="Robot in Orchard View 1">
                    <img src="/images/PhysicalTestArena.png" alt="Test Arena View 2">
                    <img src="/images/apple-detect-base.jpg" alt="Base Camera Apple Detection View 3">
                    <img src="/images/arm-drop-basket.jpg" alt="Gripper Placing Apple in Basket View 4">
                </div>
               
                <p>For this project, the task was to develop an autonomous robot that can navigate through an orchard,
                    detect apples, and pick them using a robotic arm. We developed a ROS 2–based system on the Mirte Master platform,
                    integrating self localization and mapping, dynamic obstacle avoidance, global and local path planning, apple detection (with YOLOv8 and OpenCV), 
                    gripper trajectory planning, a finite state machine for control flow, and a simple (Tkinter) graphical user interface (GUI)
                    for farmer interaction.
                    The project was validated across twelve real‐world tests, the robot passed ten of these. 
                    The passed tests highlighted <5 cm localization error, >90% apple detection 
                    accuracy within 2 m, 85–100% success in picking/placing, reliable arm retraction, and sub‐second GUI updates.
                    The unpassed tests regarding navigation and dynamic obstacle avoidance tests, showed integration limits. 
                    Autonomous navigation (via Nav2) could not be executed on hardware, leading to teleop fallback and the future proposed 
                    direction for a minimal Nav2 action server to fully integrate motion planning .</p>
            </div>


            <!-- Advanced Machine Perception -->
            <div id="adv-machine-perception" class="project">
                <h2>MSc Elective - Advanced Machine Perception</h2>
                <p class="impact"><strong><em>Enhanced obstacle detection, translating to more reliable obstacle warnings for autonomous vehicles.</em></strong></p>
                <div class="project-media">
                    <img src="/images/pointpainting.png" alt="PointPainting Fusion Visualization">
                    <img src="/images/resultsAMP.png" alt="3D Detection Results Visualization">
                </div>
                
                <p>For this elective, I enhanced a 3D object detector (CenterPoint) by fusing camera and LiDAR features. 
                    Such an object detector could be used in (autonomous) vehicles, such as cars, for obstacle avoidance.
                    The fusion of camera and LiDAR was done by painting LiDAR points with colors (features) from the camera images.
                    To enhance the performance of the 3D object detector, I implemented PolarMix-inspired data augmentations,
                    including scene swapping, instance mixing, and point filtering. 
                    Additionally, adjustments to the backbone, encoder, head, and hyperparameters were also introduced. 
                    On the View-of-Delft dataset, these enhancements resulted in an overall 10% increase in mAP (mean Average Precision) 
                    and a 13% boost for the driving corridor mAP.</p>
            </div>

            <!-- Internship at N-Sea -->
            <div id="internship-n-sea" class="project">
                <h2>MSc Internship – N-Sea</h2>
                <p class="impact"><strong><em>Fast and autonomous damage detection on underwater pipelines with video footage.</em></strong></p>
                <div class="project-media">
                    <video controls>
                        <source src="/videos/NSeaDemo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                
                <p>
                    During my internship at N-Sea, I built a Python pipeline to extract event-aligned stills/clips from remotely operated vehicle (ROV) footage, 
                    integrated optical character recognition (OCR) for kilometer-point metadata,
                    and applied DINO-based weak supervision for automated feature localization. I trained per-camera object detectors with tracking 
                    and implemented active-learning loops,
                    achieving a more consistent damage detection over 40 km of validation footage. 
                    To simplify the integration into N-Sea workflows, I designed a dockerized GUI for seamless review/export. For more information,
                    you can read the executive summary of my internship report.
                    <a href="docs/Executive_summary.pdf" target="_blank">Read the Executive Summary</a>.
                </p>
            </div>



            <!-- Deep Learning Projects -->
            <div id="deep-learning" class="project">
                <h2>MSc Electives - (Computer Vision through) Deep Learning</h2>
                <p class="impact"><strong><em>Researched different deep networks and architectures for image underwater enhancement and tomoto leaf disease detection.</em></strong></p>
                <div class="project-media">
                    <img src="/images/MFEF.png" alt="MFEF module" />
                    <img src="/images/transformer.png" alt="Basic transformer structure" />
                </div>
                
                <p>In the first project, I reproduced and extended a multi‐view underwater image enhancement pipeline in PyTorch, 
                    implementing REM, PCAM and MFF modules with custom L1 and perceptual loss functions into a MFEF module (see left image). 
                    I experimented with preprocessing and normalization strategies to mitigate color cast and channel imbalance, 
                    and performed extensive hyperparameter tuning to optimize fusion weights; full code and report are available at 
                    <a href="https://github.com/j-verhoog/Deep_Learning/tree/main">github.com/j-verhoog/Deep_Learning</a>.</p>
                <p>The second project benchmarks a Vision Transformer (see right image) vs. a convolutional network (ResNet-50) on a 10-class tomato leaf disease dataset. 
                    I compared pretrained and from-scratch models using cross-entropy loss, tracked training/inference speed and memory footprint, 
                    and demonstrated that ViT reached 99.83% accuracy while ResNet-50 offered faster inference; see detailed results at 
                    <a href="https://github.com/j-verhoog/Computer_Vision_by_DeepLearning">github.com/j-verhoog/Computer_Vision_by_DeepLearning</a>.</p>
            </div>


            <!-- Skeleton Annotation -->
            <div class="project">
                <h2>BSc Thesis - Skeleton Annotation</h2>
                <p class="impact"><strong><em>Predicting pedestrian intent, improving road safety.</em></strong></p>
                <div class="project-media">
                    <img src="/images/2d_rect_cam0_0_2773.jpg" alt="Image mmdet">
                    <img src="/images/2d_rect_cam0_0_2773_0.jpg" alt="Image cut out">
                    <img src="/images/2d_rect_cam0_0_2773_0mmpose.jpg" alt="Image mmpose">
                    <img src="/images/2d_rect_cam0_0_2773_0black.jpg" alt="Image blackened">
                </div>
                
                <p>
                    My bachelor thesis was centered around creating a customizable pipeline for annotating human skeletons in image datasets. 
                    This tool is created for researchers and developers working in computer vision, particularly in pose estimation and person/object detection. 
                    The estimation of keypoints on images could for instance be used to predict the direction and speed of walking or to predict if a person has 
                    seen the vehicle. 
                    Futhermore it could be used to create 3D meshes of the skeleton models for augmented and virtual reality.
                    It integrates seamlessly with MMDetection, MMPose and LabelMe.
                    The final pipeline includes the automated and adjustable process, anonymizing people in the dataset and different export options, 
                    also for manual adjustments.
                </p>
            </div>


            <!-- This website – single-page portfolio -->
            <div id="this-website" class="project">
                <h2>This Website </h2>
                <p class="impact"><strong><em>A self-contained project showcasing a modern, responsive portfolio built only with HTML, JavaScript, and CSS.</em></strong></p>
                <p>
                    This website is developed exclusively with static web technologies. It combines HTML for the structure, CSS for styling, and JavaScript for interactive features.
                    No external frameworks or back-end components are used, offering a lightweight and fast-loading experience.
                    All personal projects are also made of only these three technologies.
                </p>
            </div>

        </section>
    </main>

    <!-- Footer Bar -->
    <footer>
        <div class="social-links">
            <a href="https://github.com/j-verhoog" target="_blank">GitHub</a>
            <a href="https://www.linkedin.com/in/jolle-verhoog-77b9b21b6/" target="_blank">LinkedIn</a>
            <a href="mailto:permanent.beleefd6q@icloud.com">Contact</a>
        </div>
    </footer>

    <script>
      // Toggle dropdown
      const btn = document.getElementById('menu-toggle');
      const menu = document.getElementById('project-menu');
      btn.addEventListener('click', e => {
        e.stopPropagation();
        menu.classList.toggle('show');
      });
      // Close when clicking outside
      document.addEventListener('click', () => {
        menu.classList.remove('show');
      });
    </script>
</body>
</html>
