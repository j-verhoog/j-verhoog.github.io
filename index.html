<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- ADJUSTMENT: Added description meta for SEO and quick pitch -->
    <meta name="description" content="Jolle Verhoog – Robotics & Computer Vision Engineer. Explore projects in automation, intelligent systems, and computer vision.">
    <title>Jolle Verhoog – Robotics & Computer Vision Engineer</title>
    <link rel="icon" href="/images/favicon_robot.png" type="image/png">
    <link rel="stylesheet" href="/styles.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Z2E4KZ28M3"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Z2E4KZ28M3');
    </script>
</head>
<body>

    <!-- Header Bar -->
    <header>
        <div class="social-links">
            <a href="https://github.com/j-verhoog" target="_blank">GitHub</a>
            <a href="https://www.linkedin.com/in/jolle-verhoog-77b9b21b6/" target="_blank">LinkedIn</a>
            <a href="mailto:permanent.beleefd6q@icloud.com">Contact</a>
        </div>
        <div class="project-dropdown">
                <!-- ADJUSTMENT: Simplified label from 'All Projects' to 'Projects' -->
                <button id="menu-toggle" class="menu-btn">Projects ▼</button>
                <ul id="project-menu" class="dropdown-menu">
                  <li class="dropdown-header">Academic Projects</li>
                  <li><a href="#master-thesis">MSc Thesis – Federated Learning</a></li>
                  <li><a href="#apple-robot">MSc Final Project – Autonomous Apple Picking Robot</a></li>
                  <li><a href="#internship-n-sea">MSc Internship – N-Sea</a></li>
                  <li><a href="#adv-machine-perception">Advanced Machine Perception</a></li>
                  <li><a href="#deep-learning">Deep Learning (for Computer Vision)</a></li>
                  <li><a href="#skeleton-annotation">BSc Thesis – Skeleton Annotation</a></li>

                  <li class="divider"></li>
                  <li class="dropdown-header">Personal Projects</li>
                  <li><a href="/projects/skeleton-tag/">Skeleton Tag</a></li>
                  <li><a href="/projects/snake-js/">Snake Game</a></li>

                </ul>
            </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- Profile Section -->
        <section class="profile">
            <img src="/images/profile_pic.jpg" alt="Your Photo" class="profile-photo">
            <div class="profile-description">
                <h1>Jolle Verhoog</h1>
                <p>
                    Hi, I am Jolle Verhoog and welcome to my portfolio! 
                    I am a Robotics Engineer with a deep interest in automation, intelligent systems and computer vision.
                    This portfolio highlights my journey through innovative projects, demonstrating how I apply cutting-edge technologies to solve real-world challenges. 
                    <br>
                    Feel free to explore and to reach out!
                </p>
            </div>
        </section>

        <section class="projects">

            <!-- Master Thesis -->
            <div id="master-thesis" class="project">
                <h2>MSc Thesis – Federated Learning for 3D Object Detection</h2>
                <p class="impact"><strong><em>Making autonomous vehicles robust to day–night and weather changes without sharing raw data.</em></strong></p>
                
                <div class="project-media">
                    <img src="/images/fl_simple.png" alt="Federated 3D object detection setup">
                    <img src="/images/nuscenes_driving.png" alt="Day, night and weather domains">
                </div>

                <p>
                    In my MSc thesis at TU Delft’s Intelligent Vehicles group, I study how federated learning (shown on the left) can improve the robustness of 3D object detectors for autonomous driving under temporal domain shifts such as day–night and adverse weather (shown on the right). 
                    Instead of collecting all data in one place, vehicles (clients) collaboratively train a shared LiDAR–camera model while keeping raw sensor data on-device. 
                    The focus is on cross-domain perception: ensuring that cars trained in clear daylight still detect traffic participants reliably at night, in rain, fog, or snow.
                </p>

                <p>
                    Concretely, I build a federated training pipeline for a multi-modal 3D object detector and compare centralized, standard FedAvg, and normalization-based personalization strategies on corrupted versions of nuScenes. 
                    By treating normalization layers as domain adapters, I investigate how far we can push robustness to unseen conditions while keeping the backbone shared across clients. 
                    The project combines large-scale dataset preparation (synthetic weather corruptions and day–night splits), federated optimization, and detailed analysis of how normalization statistics capture domain shifts in autonomous-vehicle perception.
                </p>
            </div>





            <!-- Autonomous Apple Picking Robot -->
            <div id="apple-robot" class="project">
                <h2>MSc Final Project - Autonomous Apple Picking Robot</h2>
                <p class="impact"><strong><em>Helping farmers pick apples 24/7 without extra manual labor.</em></strong></p>
                <div class="project-media">
                    <img src="/images/JTW07211.JPG" alt="Robot in Orchard View 1">
                    <img src="/images/PhysicalTestArena.png" alt="Test Arena View 2">

                    <img src="/images/arm-drop-basket.jpg" alt="Gripper Placing Apple in Basket View 4">
                </div>
               
                <p>For this project, the task was to develop an autonomous robot that can navigate through an orchard,
                    detect apples, and pick them using a robotic arm. We developed a ROS 2–based system on the Mirte Master platform,
                    integrating self localization and mapping, dynamic obstacle avoidance, global and local path planning, apple detection (with YOLOv8 and OpenCV), 
                    gripper trajectory planning, a finite state machine for control flow, and a simple (Tkinter) graphical user interface (GUI)
                    for farmer interaction.
                    The project was validated across twelve real‐world tests, the robot passed ten of these. 
                    The passed tests highlighted <5 cm localization error, >90% apple detection 
                    accuracy within 2 m, 85–100% success in picking/placing, reliable arm retraction, and sub‐second GUI updates.
                    The unpassed tests regarding navigation and dynamic obstacle avoidance tests, showed integration limits. 
                    Autonomous navigation (via Nav2) could not be executed on hardware, leading to teleop fallback and the future proposed 
                    direction for a minimal Nav2 action server to fully integrate motion planning.</p>
            </div>


            <!-- Advanced Machine Perception -->
            <div id="adv-machine-perception" class="project">
                <h2>MSc Elective - Advanced Machine Perception</h2>
                <p class="impact"><strong><em>More reliable obstacle warnings for autonomous vehicles.</em></strong></p>
                <div class="project-media">
                    <img src="/images/pointpainting.png" alt="PointPainting Fusion Visualization">
                    <img src="/images/resultsAMP.png" alt="3D Detection Results Visualization">
                </div>
                
                <p>For this elective, I enhanced a 3D object detector (CenterPoint) by fusing camera and LiDAR features. 
                    Such an object detector could be used in (autonomous) vehicles, such as cars, for obstacle avoidance.
                    The fusion of camera and LiDAR was done by painting LiDAR points with colors (features) from the camera images.
                    To enhance the performance of the 3D object detector, I implemented PolarMix-inspired data augmentations,
                    including scene swapping, instance mixing, and point filtering. 
                    Additionally, adjustments to the backbone, encoder, head, and hyperparameters were also introduced. 
                    On the View-of-Delft dataset, these enhancements resulted in an overall 10% increase in mAP (mean Average Precision) 
                    and a 13% boost for the driving corridor mAP.</p>
            </div>

            <!-- Internship at N-Sea -->
            <div id="internship-n-sea" class="project">
                <h2>MSc Internship – N-Sea</h2>
                <p class="impact"><strong><em>Fast and autonomous damage detection on underwater pipelines with video footage.</em></strong></p>
                <div class="project-media">
                    <video controls>
                        <source src="/videos/NSeaDemo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                
                <p>
                    During my internship at N-Sea, I built a Python pipeline to extract event-aligned stills/clips from remotely operated vehicle (ROV) footage, 
                    integrated optical character recognition (OCR) for kilometer-point metadata,
                    and applied DINO-based weak supervision for automated feature localization. I trained per-camera object detectors with tracking 
                    and implemented active-learning loops,
                    achieving a more consistent damage detection over 40 km of validation footage. 
                    To simplify the integration into N-Sea workflows, I designed a dockerized GUI for seamless review/export. For more information,
                    you can read the executive summary of my internship report.
                    <a href="docs/Executive_summary.pdf" target="_blank">Read the Executive Summary</a>.
                </p>
            </div>



            <!-- Deep Learning Projects -->
            <div id="deep-learning" class="project">
                <h2>MSc Elective - (Computer Vision through) Deep Learning</h2>
                <p class="impact"><strong><em> Underwater image enhancement and tomoto leaf disease detection with various models.</em></strong></p>
                <div class="project-media">
                    <img src="/images/MFEF.png" alt="MFEF module" />
                </div>
                
                <p>In the first project, I reproduced and extended a multi‐view underwater image enhancement pipeline in PyTorch, 
                    implementing REM, PCAM and MFF modules with custom L1 and perceptual loss functions into a MFEF module (see left image). 
                    I experimented with preprocessing and normalization strategies to mitigate color cast and channel imbalance, 
                    and performed extensive hyperparameter tuning to optimize fusion weights; full code and report are available at 
                    <a href="https://github.com/j-verhoog/Deep_Learning/tree/main">github.com/j-verhoog/Deep_Learning</a>.</p>
                <p>The second project benchmarks a Vision Transformer vs. a convolutional network (ResNet-50) on a 10-class tomato leaf disease dataset. 
                    I compared pretrained and from-scratch models using cross-entropy loss, tracked training/inference speed and memory footprint, 
                    and demonstrated that ViT reached 99.83% accuracy while ResNet-50 offered faster inference; see detailed results at 
                    <a href="https://github.com/j-verhoog/Computer_Vision_by_DeepLearning">github.com/j-verhoog/Computer_Vision_by_DeepLearning</a>.</p>
            </div>


            <!-- Skeleton Annotation -->
            <div class="project">
                <h2>BSc Thesis - Skeleton Annotation</h2>
                <p class="impact"><strong><em>Improving road safety by predicting pedestrian intent.</em></strong></p>
                <div class="project-media">
                    <img src="/images/2d_rect_cam0_0_2773.jpg" alt="Image mmdet">
                    <img src="/images/2d_rect_cam0_0_2773_0.jpg" alt="Image cut out">
                    <img src="/images/2d_rect_cam0_0_2773_0mmpose.jpg" alt="Image mmpose">
                    <img src="/images/2d_rect_cam0_0_2773_0black.jpg" alt="Image blackened">
                </div>
                
                <p>
                    My bachelor thesis was centered around creating a customizable pipeline for annotating human skeletons in image datasets. 
                    This tool is created for researchers and developers working in computer vision, particularly in pose estimation and person/object detection. 
                    The estimation of keypoints on images could for instance be used to predict the direction and speed of walking or to predict if a person has 
                    seen the vehicle. 
                    Futhermore it could be used to create 3D meshes of the skeleton models for augmented and virtual reality.
                    It integrates seamlessly with MMDetection, MMPose and LabelMe.
                    The final pipeline includes the automated and adjustable process, anonymizing people in the dataset and different export options, 
                    also for manual adjustments.
                </p>
            </div>

            <!-- BSc Minor - Crack Localization -->
            <div class="project">
                <h2>BSc Minor - Airplane Crack Localization</h2>
                <p class="impact"><strong><em>Improving airplane safety by detecting and localizating sub-surface micro-cracks .</em></strong></p>

                <div class="project-media">
                    <img src="/images/mesh_cracked.png" alt="FEM mesh with crack">
                </div>

                <p>
                    As part of the Applied AI minor, I worked in a team for three months to develop a complete machine-learning pipeline for 
                    detecting and localizing sub-surface cracks in brittle aerospace materials. Over 25,000 synthetic samples with varied crack angles, positions, and load 
                    deformations were preprocessed into uniform deformation-gradient grids for downstream AI models. 
                    A sample mesh slice with a crack in it is visualized, the background color indicates the deformation gradient.
                </p>

                <p>
                    We benchmarked classical machine-learning models (KNN, SVM, Decision Trees) and built convolutional neural 
                    networks for crack classification, reaching F1-scores above 0.99. Building on this, we created 
                    regression models to estimate crack location and orientation, achieving mean absolute errors of 
                    1.27% and 1.33° respectively. The final deliverable included a modular prediction pipeline that 
                    classifies a specimen and, when a crack is present, predicts its center and angle.
                </p>
            </div>


            <!-- This website – single-page portfolio -->
            <div id="this-website" class="project">
                <h2>This Website </h2>
                <p>
                    This website, including all personal projects, is developed exclusively with HTML, CSS, and JavaScript.
                </p>
            </div>

        </section>
    </main>

    <!-- Footer Bar -->
    <footer>
        <div class="social-links">
            <a href="https://github.com/j-verhoog" target="_blank">GitHub</a>
            <a href="https://www.linkedin.com/in/jolle-verhoog-77b9b21b6/" target="_blank">LinkedIn</a>
            <a href="mailto:permanent.beleefd6q@icloud.com">Contact</a>
        </div>
    </footer>

    <script>
      // Toggle dropdown
      const btn = document.getElementById('menu-toggle');
      const menu = document.getElementById('project-menu');
      btn.addEventListener('click', e => {
        e.stopPropagation();
        menu.classList.toggle('show');
      });
      // Close when clicking outside
      document.addEventListener('click', () => {
        menu.classList.remove('show');
      });
    </script>
</body>
</html>
